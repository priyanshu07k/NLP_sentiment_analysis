{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b718067d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing blackassign0001...\n",
      "Processing blackassign0002...\n",
      "Processing blackassign0003...\n",
      "Processing blackassign0004...\n",
      "Processing blackassign0005...\n",
      "Processing blackassign0006...\n",
      "Processing blackassign0007...\n",
      "Processing blackassign0008...\n",
      "Processing blackassign0009...\n",
      "Processing blackassign0010...\n",
      "Processing blackassign0011...\n",
      "Processing blackassign0012...\n",
      "Processing blackassign0013...\n",
      "Processing blackassign0014...\n",
      "Processing blackassign0015...\n",
      "Processing blackassign0016...\n",
      "Processing blackassign0017...\n",
      "Processing blackassign0018...\n",
      "Processing blackassign0019...\n",
      "Processing blackassign0020...\n",
      "Processing blackassign0021...\n",
      "Processing blackassign0022...\n",
      "Processing blackassign0023...\n",
      "Processing blackassign0024...\n",
      "Processing blackassign0025...\n",
      "Processing blackassign0026...\n",
      "Processing blackassign0027...\n",
      "Processing blackassign0028...\n",
      "Processing blackassign0029...\n",
      "Processing blackassign0030...\n",
      "Processing blackassign0031...\n",
      "Processing blackassign0032...\n",
      "Processing blackassign0033...\n",
      "Processing blackassign0034...\n",
      "Processing blackassign0035...\n",
      "Processing blackassign0036...\n",
      "Processing blackassign0037...\n",
      "Processing blackassign0038...\n",
      "Processing blackassign0039...\n",
      "Processing blackassign0040...\n",
      "Processing blackassign0041...\n",
      "Processing blackassign0042...\n",
      "Processing blackassign0043...\n",
      "Processing blackassign0044...\n",
      "Processing blackassign0045...\n",
      "Processing blackassign0046...\n",
      "Processing blackassign0047...\n",
      "Processing blackassign0048...\n",
      "Processing blackassign0049...\n",
      "Processing blackassign0050...\n",
      "Processing blackassign0051...\n",
      "Processing blackassign0052...\n",
      "Processing blackassign0053...\n",
      "Processing blackassign0054...\n",
      "Processing blackassign0055...\n",
      "Processing blackassign0056...\n",
      "Processing blackassign0057...\n",
      "Processing blackassign0058...\n",
      "Processing blackassign0059...\n",
      "Processing blackassign0060...\n",
      "Processing blackassign0061...\n",
      "Processing blackassign0062...\n",
      "Processing blackassign0063...\n",
      "Processing blackassign0064...\n",
      "Processing blackassign0065...\n",
      "Processing blackassign0066...\n",
      "Processing blackassign0067...\n",
      "Processing blackassign0068...\n",
      "Processing blackassign0069...\n",
      "Processing blackassign0070...\n",
      "Processing blackassign0071...\n",
      "Processing blackassign0072...\n",
      "Processing blackassign0073...\n",
      "Processing blackassign0074...\n",
      "Processing blackassign0075...\n",
      "Processing blackassign0076...\n",
      "Processing blackassign0077...\n",
      "Processing blackassign0078...\n",
      "Processing blackassign0079...\n",
      "Processing blackassign0080...\n",
      "Processing blackassign0081...\n",
      "Processing blackassign0082...\n",
      "Processing blackassign0083...\n",
      "Processing blackassign0084...\n",
      "Processing blackassign0085...\n",
      "Processing blackassign0086...\n",
      "Processing blackassign0087...\n",
      "Processing blackassign0088...\n",
      "Processing blackassign0089...\n",
      "Processing blackassign0090...\n",
      "Processing blackassign0091...\n",
      "Processing blackassign0092...\n",
      "Processing blackassign0093...\n",
      "Processing blackassign0094...\n",
      "Processing blackassign0095...\n",
      "Processing blackassign0096...\n",
      "Processing blackassign0097...\n",
      "Processing blackassign0098...\n",
      "Processing blackassign0099...\n",
      "Processing blackassign0100...\n",
      "Output saved successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from textblob import TextBlob\n",
    "import re\n",
    "\n",
    "# Function to extract text from a URL\n",
    "def extract_text_from_url(url):\n",
    "    try:\n",
    "        # Request the URL\n",
    "        response = requests.get(url)\n",
    "        # Parse the HTML content\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        # Find the article title\n",
    "        title = soup.title.text.strip()\n",
    "        # Find all paragraph tags within the article\n",
    "        paragraphs = soup.find_all('p')\n",
    "        # Concatenate the text from all paragraphs\n",
    "        text = ' '.join([p.text for p in paragraphs])\n",
    "        return title, text\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while extracting text from {url}: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# Function to perform text analysis\n",
    "def perform_text_analysis(text):\n",
    "    try:\n",
    "        # Create a TextBlob object\n",
    "        blob = TextBlob(text)\n",
    "        # Sentiment analysis\n",
    "        polarity_score = blob.sentiment.polarity\n",
    "        subjectivity_score = blob.sentiment.subjectivity\n",
    "        # Tokenization\n",
    "        words = blob.words\n",
    "        sentences = blob.sentences\n",
    "        # Calculate average sentence length\n",
    "        avg_sentence_length = sum(len(sentence.split()) for sentence in sentences) / len(sentences)\n",
    "        # Calculate percentage of complex words\n",
    "        complex_words = [word for word in words if syllable_count(word) > 2]\n",
    "        percentage_complex_words = (len(complex_words) / len(words)) * 100\n",
    "        # Calculate Fog index\n",
    "        fog_index = 0.4 * (avg_sentence_length + percentage_complex_words)\n",
    "        # Calculate average number of words per sentence\n",
    "        avg_words_per_sentence = len(words) / len(sentences)\n",
    "        # Calculate complex word count\n",
    "        complex_word_count = len(complex_words)\n",
    "        # Word count\n",
    "        word_count = len(words)\n",
    "        # Calculate syllable per word\n",
    "        syllable_per_word = sum(syllable_count(word) for word in words) / len(words)\n",
    "        # Personal pronouns count\n",
    "        personal_pronouns = count_personal_pronouns(text)\n",
    "        # Average word length\n",
    "        avg_word_length = sum(len(word) for word in words) / len(words)\n",
    "        # Positive and Negative score\n",
    "        positive_score = sum(1 for sentence in blob.sentences if sentence.sentiment.polarity > 0)\n",
    "        negative_score = sum(1 for sentence in blob.sentences if sentence.sentiment.polarity < 0)\n",
    "        return polarity_score, subjectivity_score, avg_sentence_length, percentage_complex_words, \\\n",
    "               fog_index, avg_words_per_sentence, complex_word_count, word_count, syllable_per_word, \\\n",
    "               personal_pronouns, avg_word_length, positive_score, negative_score\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during text analysis: {e}\")\n",
    "        return None, None, None, None, None, None, None, None, None, None, None, None, None, None\n",
    "\n",
    "# Function to count syllables in a word\n",
    "def syllable_count(word):\n",
    "    vowels = 'aeiouy'\n",
    "    word = word.lower()\n",
    "    count = 0\n",
    "    if word[0] in vowels:\n",
    "        count += 1\n",
    "    for index in range(1, len(word)):\n",
    "        if word[index] in vowels and word[index - 1] not in vowels:\n",
    "            count += 1\n",
    "    if word.endswith('e'):\n",
    "        count -= 1\n",
    "    if count == 0:\n",
    "        count += 1\n",
    "    return count\n",
    "\n",
    "# Function to count personal pronouns in text\n",
    "def count_personal_pronouns(text):\n",
    "    pronouns = ['I', 'me', 'my', 'mine', 'myself', 'we', 'us', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves']\n",
    "    count = sum(1 for word in re.findall(r'\\b\\S+\\b', text) if word.lower() in pronouns)\n",
    "    return count\n",
    "\n",
    "# Load URLs from input file\n",
    "input_file = r'C:\\Users\\dhima\\Desktop\\Input.xlsx'\n",
    "df_urls = pd.read_excel(input_file)\n",
    "\n",
    "# Initialize lists to store output data\n",
    "output_data = []\n",
    "\n",
    "# Iterate through each URL\n",
    "for index, row in df_urls.iterrows():\n",
    "    url_id = row['URL_ID']\n",
    "    url = row['URL']\n",
    "    print(f\"Processing {url_id}...\")\n",
    "    # Extract text from the URL\n",
    "    title, text = extract_text_from_url(url)\n",
    "    if title and text:\n",
    "        # Perform text analysis\n",
    "        polarity_score, subjectivity_score, avg_sentence_length, percentage_complex_words, \\\n",
    "        fog_index, avg_words_per_sentence, complex_word_count, word_count, syllable_per_word, \\\n",
    "        personal_pronouns, avg_word_length, positive_score, negative_score = perform_text_analysis(text)\n",
    "        if all(v is not None for v in [polarity_score, subjectivity_score, avg_sentence_length, percentage_complex_words,\n",
    "                                       fog_index, avg_words_per_sentence, complex_word_count, word_count,\n",
    "                                       syllable_per_word, personal_pronouns, avg_word_length, positive_score, negative_score]):\n",
    "            # Append data to the output list\n",
    "            output_data.append([url_id, url, title, positive_score, negative_score, polarity_score, subjectivity_score, avg_sentence_length,\n",
    "                                percentage_complex_words, fog_index, avg_words_per_sentence, complex_word_count,\n",
    "                                word_count, syllable_per_word, personal_pronouns, avg_word_length])\n",
    "        else:\n",
    "            print(f\"Error: Text analysis failed for {url_id}\")\n",
    "    else:\n",
    "        print(f\"Error: Failed to extract text for {url_id}\")\n",
    "\n",
    "# Create DataFrame for output\n",
    "output_columns = [\"URL_ID\", \"URL\", \"Title\", \"Positive Score\", \"Negative Score\", \"POLARITY SCORE\", \"SUBJECTIVITY SCORE\", \"AVG SENTENCE LENGTH\",\n",
    "                  \"PERCENTAGE OF COMPLEX WORDS\", \"FOG INDEX\", \"AVG NUMBER OF WORDS PER SENTENCE\",\n",
    "                  \"COMPLEX WORD COUNT\", \"WORD COUNT\", \"SYLLABLE PER WORD\", \"PERSONAL PRONOUNS\",\n",
    "                  \"AVG WORD LENGTH\"]\n",
    "df_output = pd.DataFrame(output_data, columns=output_columns)\n",
    "\n",
    "# Save output to Excel file\n",
    "output_file = \"Output.xlsx\"\n",
    "df_output.to_excel(output_file, index=False)\n",
    "print(\"Output saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a2167a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14724db9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
